{
  "model": {
    "type": "T5LoRA",
    "args": {
      "t5_config": {
        "model_name": "t5-base",
        "cache_dir": "cache/",
        "max_length": 1024,
        "max_new_tokens": 256
      },
      "lora_config": {
        "rank": 4, 
        "alpha": 16, 
        "dropout_p": 0.1,
        "target_layers": ["q", "k", "v", "o", "wi", "wo"]
      }
    }
  },
  "trainer": {
    "wandb_run_name": "[GLUE] T5 Single LoRA 3e bs2+ga4"
  }
}