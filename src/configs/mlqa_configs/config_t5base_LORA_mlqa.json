{
  "name": "T5_CL_summarization",
  "n_gpu": 1,
  "preprocessing": {},
  "model": {
    "type": "T5LoRA",
    "args": {
      "t5_config": {
        "model_name": "google/mt5-base",
        "cache_dir": "cache/",
        "max_length": 1024
      },
      "lora_config": {
        "rank": 4, 
        "alpha": 16, 
        "dropout_p": 0.1,
        "target_layers": ["q", "k", "v", "o", "wi", "wo"]
      }
    }
  },
  "optimizer": {
    "type": "AdamW",
    "args": {
      "lr": 5e-4,
      "betas": [0.9, 0.95],
      "weight_decay": 1e-5
    }
  },
  "metrics": [
    {
        "type": "ExactMatch_MLQAMetric",
        "args": {"name": "Exact-Match", "model_type": "enc-dec"}
    },
    {
        "type": "F1_MLQAMetric",
        "args": {"name": "F1", "model_type": "enc-dec"}
    }
  ],
  "lr_scheduler": {
    "type": "ExponentialLR",
    "args": {
      "gamma": 1.0
    }
  },
  "loss": {
    "type": "CrossEntropyLoss",
    "args": {}
  },
  "trainer": {
    "epochs": 35,
    "eval_frequency": 1,
    "first_epoch_eval_only": true,
    "grad_norm_clip": 100.0,
    "grad_accum_steps": 4,
    "mixed_precision": "no",
    "save_dir": "saved/",
    "save_period": 60,
    "verbosity": 2,
    "monitor": "min loss",
    "visualize": "wandb",
    "wandb_project": "nlp-cl-project",
    "wandb_run_name": "[MLQA] mT5 Single LoRA 5e bs2+ga4"
  }
}